{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 2 µs, total: 7 µs\n",
      "Wall time: 12.9 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "import time\n",
    "# create a browser instance\n",
    "from selenium import webdriver\n",
    "# emulate keyboard inputs\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "# creatinga single browser instance\n",
    "import selenium.webdriver.firefox.service as service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "# WebDriverWait and EC to allow waiting for element to load on page\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "# module to search for elements using xpaths\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "# exception handling\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "# quick clicking and scrolling\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "# searching of html with \"find()\"\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import sys\n",
    "import math\n",
    "import os                       # file saving\n",
    "import datetime\n",
    "import re\n",
    "import unidecode                # to remove accents\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Selenium Bot Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Make sure that \"chromedriver\" and \"geckodriver\" are in this directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 8.11 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "class selenium_bot():\n",
    "    \"\"\"\n",
    "    Interactable bot, that parses outlook files\n",
    "    \"\"\"\n",
    "    def __init__(self, browser, timeout, save_period, url, page_loaded_xpath):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [str] browser: \"Firefox\" or \"Chrome\"\n",
    "        [float] timeout: how long to wait for responses\n",
    "        [save_period] float: time in seconds to create backup of parsed data\n",
    "        [str] url: url bot starts off at\n",
    "        [str] page_loaded_xpath: xpath to indicate that page has loaded\n",
    "\n",
    "        __ Description __\n",
    "        sets up selenium bot\n",
    "        \"\"\"\n",
    "\n",
    "        self.browser = browser.lower()\n",
    "        self.timeout = timeout\n",
    "        self.url = url\n",
    "        self.page_loaded_xpath = page_loaded_xpath\n",
    "        \n",
    "        # 1 - setup browser\n",
    "        print(\"==> setup_browser start\")\n",
    "        if(self.browser == \"firefox\"):\n",
    "            self.driver = self.__setup_firefox()\n",
    "        else:\n",
    "            self.driver = self.__setup_chrome()\n",
    "        self.driver.maximize_window()\n",
    "\n",
    "        # 2 - load page\n",
    "        self.driver.get(self.url)\n",
    "\n",
    "        # 3- supprorting parameters for the future\n",
    "        # waiter, to wait for contents to load. call the \"waiter.until(function)\" method\n",
    "        self.WebDriverWaiter = WebDriverWait(self.driver, self.timeout)\n",
    "        self.save_period = save_period\n",
    "        \n",
    "        print(\"==> setup_browser end\\n\")\n",
    "\n",
    "    def __setup_firefox(self):\n",
    "        \"\"\"\n",
    "        __ Description __\n",
    "        open up a firefox driver\n",
    "\n",
    "        __ Returns __\n",
    "        driver handle\n",
    "        \"\"\"\n",
    "\n",
    "        # 1 - create a browser instance\n",
    "        print(\"  > Starting new Firefox server\")\n",
    "        browser = webdriver.Firefox(\n",
    "            executable_path='./geckodriver')\n",
    "\n",
    "        return browser\n",
    "\n",
    "    def __setup_chrome(self):\n",
    "        \"\"\"\n",
    "        __ Description __\n",
    "        open up a chrome driver\n",
    "\n",
    "        __ Returns __\n",
    "        driver handle\n",
    "        \"\"\"\n",
    "\n",
    "        # 1 - set capabilities\n",
    "        capabilities = {'chromeOptions':\n",
    "                        {\n",
    "                            'useAutomationExtension': False,\n",
    "                            'args': ['--disable-extensions']}\n",
    "                        }\n",
    "\n",
    "        # 2 - set options for chrome\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_experimental_option(\"prefs\", {\n",
    "            \"download.prompt_for_download\": False,\n",
    "            \"download.directory_upgrade\": True,\n",
    "            \"safebrowsing.enabled\": True\n",
    "        })\n",
    "\n",
    "        # 3 - create a browser instance with defined options\n",
    "        print(\"  > Starting new Chrome server\")\n",
    "        browser = webdriver.Chrome(executable_path=\"./chromedriver\",\n",
    "                                   desired_capabilities=capabilities,\n",
    "                                   options=chrome_options)\n",
    "        return browser\n",
    "    \n",
    "    def supp_extract_html(self, soup, html_tags_array):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [soup] soup: html to extract from formatted with BeautifulSoup\n",
    "        html_tags_array: array of the form\n",
    "        \n",
    "        [[\"div\", {\"role\": \"option\"}], \n",
    "        [\"div\", {\"aria-label\": \"Reading Pane\"}], \n",
    "        ...]\n",
    "\n",
    "        which specifies the name (\"div\", \"span\") and attributes ({\"id\": [\"test1\", \"test2\"], \"aria-label\": \"pane\"})\n",
    "        from outer to inner tags, iteratively going down specificity levels\n",
    "\n",
    "        __ Description __\n",
    "        iterates through the supplied \"soup\" html looking for tags whose parrents match all the supplied \"html_tags\"\n",
    "\n",
    "        __ Return __\n",
    "        [htmltag1, htmltag2, htmltag3]: array of html tags that fit the search requirement\n",
    "        \"\"\"\n",
    "\n",
    "        structure_depth  = len(html_tags_array)\n",
    "        debug_counter = 0\n",
    "\n",
    "        try:\n",
    "            if(structure_depth != 1):\n",
    "                # 1 - unpack the first structure\n",
    "                current_structure = soup.find(\n",
    "                    html_tags_array[0][0], attrs=html_tags_array[0][1])\n",
    "\n",
    "                # 2 - unpack further structures until we get to the last one\n",
    "                for i in range(1, structure_depth - 1):\n",
    "                    debug_counter += 1\n",
    "                    name = html_tags_array[i][0]\n",
    "                    attrs = html_tags_array[i][1]\n",
    "                    current_structure = current_structure.find(names, attrs=attrs)\n",
    "\n",
    "                # 3 - extract all matches from the lowest structure\n",
    "                current_structure = current_structure.find_all(\n",
    "                    html_tags_array[-1][0], attrs=html_tags_array[-1][1])\n",
    "            else:\n",
    "                # 1 - in the special case that only one structure is specified\n",
    "                current_structure = soup.find_all(\n",
    "                    html_tags_array[0][0], attrs=html_tags_array[0][1])\n",
    "\n",
    "            return current_structure\n",
    "            \n",
    "        except AttributeError:\n",
    "            # Error when an entry is missing\n",
    "            print(\"The page does not have the html element:\\n\\t[%s, %s]\"\n",
    "                  % (html_tags_array[debug_counter], html_tags_array[debug_counter]))\n",
    "            \n",
    "            return \"\"\n",
    "        \n",
    "    def supp_extract_text(self, soup, html_tags_array):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [soup] soup: html to extract from formatted with BeautifulSoup\n",
    "        html_tags_array: array of the form\n",
    "        \n",
    "        [[\"div\", {\"role\": \"option\"}], \n",
    "        [\"div\", {\"aria-label\": \"Reading Pane\"}], \n",
    "        ...]\n",
    "\n",
    "        which specifies the name (\"div\", \"span\") and attributes ({\"id\": [\"test1\", \"test2\"], \"aria-label\": \"pane\"})\n",
    "        from outer to inner tags, iteratively going down specificity levels\n",
    "\n",
    "        __ Description __\n",
    "        iterates through the supplied \"soup\" html looking for tags whose parrents match all the supplied \"html_tags\"\n",
    "        then a text array is extracted from this tag\n",
    "\n",
    "        __ Return __\n",
    "        [array] matching text in the innter structure\n",
    "        \"\"\"\n",
    "\n",
    "        html_structure = self.supp_extract_html(soup, html_tags_array)\n",
    "        \n",
    "        # 1 - take all of the tags found and extract text\n",
    "        array_to_return = [i.get_text().strip() for i in html_structure]\n",
    "        \n",
    "        return array_to_return\n",
    "        \n",
    "    def supp_write_to_element(self, element_xpath, fill_value):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [str] element_xpath: element to look for e.g. //div[@id=|password|]\n",
    "        [str] fill_value: what to write in the form\n",
    "\n",
    "        __ Description __\n",
    "        enters the \"fill_value\" into the chosen \"element\"\n",
    "        \"\"\"\n",
    "        self.supp_wait_for_xpath(element_xpath)\n",
    "        \n",
    "        element = self.driver.find_element_by_xpath(element_xpath)\n",
    "        if(element):\n",
    "            element.send_keys(fill_value)\n",
    "        else:\n",
    "            print(\"**> Element with xpath %s does not exist\" %element_xpath)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def supp_wait_for_xpath(self, xpath):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [str] xpath: xpath to wait for\n",
    "\n",
    "        __ Description __\n",
    "        pauses the browser until \"xpath\" is loaded on the page\n",
    "        \"\"\"\n",
    "        self.WebDriverWaiter.until(\n",
    "            EC.presence_of_element_located(\n",
    "                (By.XPATH, xpath)), \n",
    "            message=\"Did not find %s within the timeout time you set of %i\"%(xpath, self.timeout)\n",
    "        )\n",
    "        \n",
    "    def supp_click(self, xpath):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [str] xpath: xpath of object to click\n",
    "\n",
    "        __ Description __\n",
    "        clicks the element\n",
    "        \"\"\"\n",
    "        self.driver.find_element_by_xpath(xpath).click()\n",
    "\n",
    "        \n",
    "    def supp_load_soup(self):\n",
    "        \"\"\"\n",
    "        Loads up a soup of all the html on the visible page\n",
    "        __ Returns __\n",
    "        Soup Object to search\n",
    "        \"\"\"\n",
    "        html = self.driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        return soup\n",
    "    \n",
    "    def refresh(self):\n",
    "        \"\"\"\n",
    "        __ Description __\n",
    "        Resets variables of bot class and reload page\n",
    "        \"\"\"\n",
    "        self.pandas_out = pd.DataFrame(columns=self.pandas_out.columns)\n",
    "        running_class.driver.get(self.url)\n",
    "        self.supp_wait_for_xpath(self.page_loaded_xpath)\n",
    "\n",
    "    def supp_save_data(self, file_name=\"pandas_out\", ext=\"csv\"):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [str] file_name: the file to save to. provide .pkl or .csv extension\n",
    "        \n",
    "        __ Description __\n",
    "        Saves data accumulated in \"pandas_out\" to output file\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1 - create output directory\n",
    "        if not os.path.exists(\"./output\"):\n",
    "            os.mkdir(\"output\")\n",
    "\n",
    "        # 2 - cut any extensions that were given by accident\n",
    "        file_name = file_name.split(\".\")[0]\n",
    "        file_name = \"./output/%s\" % (file_name)\n",
    "        \n",
    "        if(ext == \"pkl\"):\n",
    "            self.pandas_out.to_pickle(\"%s.pkl\" % file_name)\n",
    "        else:\n",
    "            self.pandas_out.to_csv(\"%s.csv\" % file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1e+03 ns, total: 6 µs\n",
      "Wall time: 9.78 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "def wikipedia_scrape_words():\n",
    "    # 1 click random article\n",
    "    running.driver.find_element_by_xpath(\"//li[@id = 'n-randompage']\").click()\n",
    "    \n",
    "    # 2 exract all text\n",
    "    soup = running.supp_load_soup()\n",
    "    page_title = soup.find(\"h1\", attrs = {\"id\": \"firstHeading\"}).get_text()\n",
    "    page_content = soup.find(\"div\", attrs = {\"class\": \"mw-content-ltr\"}).get_text()\n",
    "    page_text = page_content.split(\" \")\n",
    "    \n",
    "    # 3 -filter only words with characters a-z and before refferences\n",
    "    page_text_filtered = set()\n",
    "    for i in page_text:\n",
    "\n",
    "        # do not look at beyond refferences\n",
    "        mg_refference = re.search(\".*References\\\\[edit\\\\].*\",i, flags = re.S)\n",
    "        if(mg_refference):\n",
    "            break\n",
    "\n",
    "        # do not keep words with capitalized letters inthe middle or numbers\n",
    "        mg_Capital = re.findall(\"(\\s|^)((\\w+[A-Z]\\w+)|(.*\\d.*))(\\s|$)\", i)\n",
    "        if(mg_Capital):\n",
    "            pass\n",
    "        else:\n",
    "            # search for full words of length 2-15 and exclude square brackets\n",
    "            mg = re.search(\"(\\s|^)(\\w{2,15})(\\\\[\\d+\\\\])?(\\s|$)\", i)\n",
    "            if(mg):\n",
    "                filtered = mg.group(2).lower()\n",
    "                page_text_filtered.add(filtered)\n",
    "\n",
    "    return set(page_text_filtered), page_title\n",
    "            \n",
    "def convert_to_nn_format(set_to_convert):\n",
    "    \"\"\"\n",
    "    __ Parameters __\n",
    "    set_to_convert\n",
    "\n",
    "    __ Description __\n",
    "    converts the supplied set by\n",
    "    - trimming to 15 characters\n",
    "    - removing all accents\n",
    "    - lowercasing\n",
    "\n",
    "\n",
    "    __ Return __\n",
    "    converted_set, number_of_deletions\n",
    "    \"\"\"\n",
    "    converted_set = []\n",
    "\n",
    "    for i in set_to_convert:\n",
    "        # only words 2-15 kept\n",
    "        mg = re.search(\"(\\w{2,15})\", i)\n",
    "        if(mg):\n",
    "            # lower case\n",
    "            i = i.lower()\n",
    "\n",
    "            # remove accents\n",
    "            i = unidecode.unidecode(i)\n",
    "\n",
    "            converted_set.append(i)\n",
    "\n",
    "    return set(converted_set)\n",
    "\n",
    "class wait_for_change():\n",
    "  \"\"\"Wait for the content of the specified locator to change\n",
    "\n",
    "  To be used in the following way:\n",
    "  formWebDriverWait.until(wait_for_content_loaded())\n",
    "  \"\"\"\n",
    "  def __init__(self, locator, old_value):\n",
    "      self.locator = locator\n",
    "      self.old_value = old_value\n",
    "  \n",
    "  def __call__(self, driver):\n",
    "\n",
    "    try:\n",
    "        present = EC._find_element(driver, self.locator).text.strip()\n",
    "        present_changed = ((present != \"\" ) and (present != self.old_value))\n",
    "        \n",
    "        return present_changed\n",
    "    \n",
    "    except StaleElementReferenceException:\n",
    "        return False\n",
    "\n",
    "def write_set(set_to_write, name):\n",
    "    \"\"\"\n",
    "    __ Parameters __\n",
    "    [str] name: name of output file\n",
    "    [set] set_to_write: set to write to file\n",
    "\n",
    "    __ Description __\n",
    "    write to file in format\n",
    "    word, unique id\n",
    "    \"\"\"\n",
    "\n",
    "    fout = open(name,\"a\")\n",
    "    set_list = list(set_to_write)[:10000]\n",
    "\n",
    "    for i in set_list:\n",
    "        unique_id = random.randrange(1**10, 9 * 10**10)\n",
    "        to_write = \"%s, %i\\n\" %(i, unique_id)\n",
    "        fout.write(to_write)\n",
    "\n",
    "    fout.close()\n",
    "    print(\"==> Wrote set to file %s.txt\" %(name))\n",
    "\n",
    "def refine_file(file_to_edit):\n",
    "    \"\"\"\n",
    "    __ Parameters __\n",
    "    file_to_edit\n",
    "\n",
    "    __ Description __\n",
    "    chinese words had apostophese in them like as mashao'er. this method remvoes them\n",
    "    \"\"\"\n",
    "\n",
    "    data_array = []\n",
    "    with open(file_to_edit, \"r\") as fin:\n",
    "        for line in fin:\n",
    "            data_array.append(re.sub(\"'|\\\"\", \"\", line))\n",
    "\n",
    "    with open(file_to_edit, \"w\") as fout:\n",
    "        for i in data_array:\n",
    "            fout.write(\"%s\" %i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Browser instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> setup_browser start\n",
      "  > Starting new Chrome server\n",
      "==> setup_browser end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "########################################\n",
    "timeout=10                      # seconds to wait for page elements to load before quitting\n",
    "browser=\"chrome\"                # firefox of chrome\n",
    "url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "########################################\n",
    "########################################\n",
    "running = selenium_bot(browser, timeout, None, url, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## English Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53/10000]\tAlfred Liebster\n",
      "[144/10000]\tThe Parks (painting)\n",
      "[465/10000]\tGirl Crazy\n",
      "[628/10000]\tMarino Institute of Education\n",
      "[667/10000]\tIkeda Tsugumasa\n",
      "[686/10000]\tSixcyl\n",
      "[867/10000]\tMariana Meerhoff\n",
      "[935/10000]\tElectoral district of Wollongong\n",
      "[1028/10000]\tRadó von Kövesligethy\n",
      "[1181/10000]\tThorngumbald\n",
      "[1298/10000]\tSt. Ann Church (Manhattan)\n",
      "[1366/10000]\tCoppenhall High School\n",
      "[1386/10000]\tShimin Byōin-mae Station (Toyama)\n",
      "[1570/10000]\tAllison McGourty\n",
      "[1680/10000]\tList of horror films of 1960\n",
      "[1791/10000]\tNeha Bam\n",
      "[1809/10000]\tHamoud Muhammed Ou'bad\n",
      "[1816/10000]\tAnton Byström\n",
      "[1861/10000]\tClandown F.C.\n",
      "[1886/10000]\tZealand Air Defence Regiment\n",
      "[1996/10000]\tValentine Pelka\n",
      "[2137/10000]\tPrivatization in criminal justice\n",
      "[2165/10000]\tChester Noyes Greenough\n",
      "[2345/10000]\tDairy Council of California\n",
      "[2414/10000]\tChichester Bell\n",
      "[2495/10000]\tKjetil André Aamodt\n",
      "[2536/10000]\tMouhcine Cheaouri\n",
      "[2565/10000]\tChoondacherry\n",
      "[2572/10000]\tCeleste Poltera\n",
      "[2637/10000]\tMatthias Vehe\n",
      "[2640/10000]\tAlex Perry (table tennis)\n",
      "[2659/10000]\tPlay.Rock.Music\n",
      "[2684/10000]\tVoice of the People of Tunisia\n",
      "[2690/10000]\tAhmadi, Bushehr\n",
      "[2809/10000]\tEffect of the 2004 Indian Ocean earthquake on Somalia\n",
      "[2851/10000]\tHennepin Center for the Arts\n",
      "[2913/10000]\tNorthfield Union of Youth\n",
      "[2984/10000]\tViolet sabrewing\n",
      "[3041/10000]\tRoosevelt County, Montana\n",
      "[3122/10000]\tIngrid Sheldon\n",
      "[3306/10000]\tThomas Vowler Short\n",
      "[3327/10000]\tRene Denfeld\n",
      "[3386/10000]\tLewis Chalmers\n",
      "[3405/10000]\tA. H. Patch\n",
      "[3576/10000]\tFrederick May (composer)\n",
      "[3660/10000]\tTiger reserves of India\n",
      "[3695/10000]\tEzibeleni\n",
      "[3709/10000]\tSt. Stephen Church (Trumbull, Connecticut)\n",
      "[3717/10000]\tAegila\n",
      "[3735/10000]\tScratch My Arse Rock\n",
      "[3755/10000]\tİpek Özgan\n",
      "[3789/10000]\tThe Immaculate Consumptive\n",
      "[3935/10000]\tProgressive Republicans (France)\n",
      "[3940/10000]\tAdams Township, Ripley County, Indiana\n",
      "[4098/10000]\tSignorelli parapraxis\n",
      "[4112/10000]\tGensou no Hana\n",
      "[4119/10000]\tJola-Felupe language\n",
      "[4126/10000]\tMelanella aethiopica\n",
      "[4165/10000]\tCaptain Fantastic (TV series)\n",
      "[4212/10000]\tMajid Jalali\n",
      "[4307/10000]\tNut roast\n",
      "[4338/10000]\tSuriname at the 2015 World Championships in Athletics\n",
      "[4349/10000]\tJournal of Animal Science\n",
      "[4363/10000]\tIssa Kallon\n",
      "[4415/10000]\tStephen Shaw (ombudsman)\n",
      "[4539/10000]\tArtem Milevskiy\n",
      "[4622/10000]\tPenguin Celebrations\n",
      "[4747/10000]\tCentury Gothic\n",
      "[4790/10000]\tEdward Rowland Sill\n",
      "[4800/10000]\tLuo Zhichuan\n",
      "[4806/10000]\tYoyaga Dit Coulibaly\n",
      "[4859/10000]\tPolk Museum of Art\n",
      "[4866/10000]\tKalateh-ye Mazar, Razavi Khorasan\n",
      "[4892/10000]\tLasthenia minor\n",
      "[5031/10000]\tAnne Meson\n",
      "[5131/10000]\tGlan-Blies Way\n",
      "[5144/10000]\tTyphoon Faxai\n",
      "[5248/10000]\tJingle\n",
      "[5252/10000]\tGifan Rural District\n",
      "[5261/10000]\tICI-190,622\n",
      "[5365/10000]\tLRRC57\n",
      "[5436/10000]\tInternational Communist Seminar\n",
      "[5454/10000]\tF.W. Johnson Collegiate\n",
      "[5457/10000]\tBezakshan\n",
      "[5790/10000]\tSwan Lake (Bourne)\n",
      "[5804/10000]\tRhoptropella\n",
      "[5856/10000]\tLuise Begas-Parmentier\n",
      "[5965/10000]\tGeorge Smith (royal servant)\n",
      "[5985/10000]\tTwisted by Design\n",
      "[6186/10000]\tSchneider Trophy\n",
      "[6697/10000]\tGalactic Empire (Isaac Asimov)\n",
      "[6723/10000]\t2012 FIBA EuroChallenge Final Four\n",
      "[6743/10000]\tNyssa sinensis\n",
      "[6793/10000]\tLa Ley discography\n",
      "[6959/10000]\tRatangarh, Churu\n",
      "[6972/10000]\tBorceguí Island\n",
      "[7055/10000]\tHatch Mott MacDonald\n",
      "[7081/10000]\tHelge Boes\n",
      "[7101/10000]\tGignac Bridge\n",
      "[7103/10000]\tDo Shakh, Afghanistan\n",
      "[7105/10000]\tDietmar Thöni\n",
      "[7119/10000]\t1874 English cricket season\n",
      "[7251/10000]\tMørefly\n",
      "[7257/10000]\tFonoti (surname)\n",
      "[7584/10000]\tHistory of the Squamish people\n",
      "[7589/10000]\tKhökh Serkh\n",
      "[7596/10000]\tIndika Bandaranayake\n",
      "[7608/10000]\t1961 UCI Road World Championships – Men's road race\n",
      "[7738/10000]\tRoald Jensen\n",
      "[7759/10000]\tSmith & Thell\n",
      "[7794/10000]\tVic So'oto\n",
      "[7888/10000]\t1991 Asturian regional election\n",
      "[7952/10000]\tTelevisió de Catalunya\n",
      "[8051/10000]\tChen Wen Hsi\n",
      "[8075/10000]\tGreatest Hits (Dan Fogelberg album)\n",
      "[8211/10000]\tUnited States Coast Guard Sector\n",
      "[8323/10000]\t2nd Infantry Division Sforzesca\n",
      "[8378/10000]\tEric Bentley\n",
      "[8387/10000]\tKerry–Meath Gaelic football rivalry\n",
      "[8505/10000]\tMade World Tour\n",
      "[8541/10000]\tDaniel Giacomino\n",
      "[8782/10000]\tGovernment of Croatia\n",
      "[8843/10000]\tCommittee for the Re-Election of the President\n",
      "[8882/10000]\tDick Taverne\n",
      "[8901/10000]\tPheretima praepinguis\n",
      "[8912/10000]\tNandimithra Ekanayake\n",
      "[8934/10000]\tBesse Day\n",
      "[9013/10000]\tDowntown Seattle\n",
      "[9036/10000]\tObey Giant (film)\n",
      "[9069/10000]\tChanda Mwaba\n",
      "[9176/10000]\t2009 Hong Kong Cricket Sixes\n",
      "[9182/10000]\tSalaq-e Nuri\n",
      "[9267/10000]\tBen Aqua\n",
      "[9282/10000]\tPriyankar Mukherjee\n",
      "[9333/10000]\tLuke Baines\n",
      "[9380/10000]\tPrelims\n",
      "[9809/10000]\tMombasa\n",
      "[9965/10000]\tSleepers of Mars\n",
      "[9983/10000]\tKilburn Park tube station\n",
      "[9997/10000]\tSovietization\n",
      "[10007/10000]\tLeft for Dead (Wussy album)\n",
      "==> Scraped 141 article(s) for 10007 elements\n",
      "==> Wrote set to file ./output/english.txt.txt\n"
     ]
    }
   ],
   "source": [
    "running.driver.get(\"https://en.wikipedia.org/wiki/Main_Page\")\n",
    "target = 10000\n",
    "english_set = set()\n",
    "english_articles = []\n",
    "file_name_to_output = \"./output/english.txt\"\n",
    "# file_name_to_output = \"./output/english_chinese_2.txt\" this is for the chinese words\n",
    "\n",
    "while(len(english_set) < target):\n",
    "    # collect words from page\n",
    "    wiki_words, title = wikipedia_scrape_words()\n",
    "    english_articles.append(title)\n",
    "    \n",
    "    # filter for length 2-15 and accents and add to set\n",
    "    converted_words = convert_to_nn_format(wiki_words)\n",
    "    english_set = english_set.union(converted_words)\n",
    "\n",
    "    print(\"[%i/%i]\\t%s\" %(len(english_set), target, title))\n",
    "\n",
    "print(\"==> Scraped %i article(s) for %i elements\" %(len(english_articles),len(english_set)))\n",
    "write_set(english_set, file_name_to_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Chinese set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "english_file_to_use = \"./output/english_chinese2.txt\"\n",
    "no_sections_to_split_into = 100 # page slows down after a few traslations, so food to reload it\n",
    "\n",
    "# 1 - set to translate from english to chinese\n",
    "running.driver.get(\"https://translate.google.com/\")\n",
    "running.supp_click(\"//div[@class='sl-wrap']/div/div[2]/div[@value='en']\")\n",
    "running.supp_click(\"//div[@class='tl-wrap']/div[@aria-label='More']\")\n",
    "try:\n",
    "    running.supp_click(\n",
    "        \"//div[@class='language_list_item_wrapper language_list_item_wrapper-zh-TW']\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 2 - load up file with english words (generated above)\n",
    "english_for_chinese_set = []\n",
    "with open(english_file_to_use, \"r\") as fin:\n",
    "    for line in fin:\n",
    "        english_for_chinese_set.append(line.split(\",\")[0])\n",
    "\n",
    "\n",
    "# 3 - split the word up into sections\n",
    "length_english_set = len(english_for_chinese_set)\n",
    "section_length = int(length_english_set / no_sections_to_split_into)\n",
    "print(\"==> Transalting English. Using %i sections x %i words each for a total of %i words\" %\n",
    "      (no_sections_to_split_into, section_length, no_sections_to_split_into * section_length))\n",
    "\n",
    "\n",
    "for section in range(0, no_sections_to_split_into):\n",
    "\n",
    "    # a - set section variables\n",
    "    begin = section * section_length\n",
    "    end = (section + 1) * section_length\n",
    "    english_set_section = list(english_for_chinese_set)[begin:end]\n",
    "    translation_old = \"\"\n",
    "    chinese_set = set()\n",
    "\n",
    "    for i, english_word in enumerate(english_set_section):\n",
    "        # b - write word to translate\n",
    "        running.supp_write_to_element(\"//textarea\", english_word)\n",
    "\n",
    "        # c - wait for translation to load\n",
    "        running.WebDriverWaiter.until(wait_for_change((By.XPATH,\n",
    "                                                       \"//div[@class='tlid-result-transliteration-container result-transliteration-container transliteration-container']\"), translation_old))\n",
    "\n",
    "        # d - extract translation\n",
    "        translation = running.supp_extract_text(running.supp_load_soup(),\n",
    "                                                [[\"div\",\n",
    "                                                  {\"class\": \"tlid-result-transliteration-container result-transliteration-container transliteration-container\"}],\n",
    "                                                 [\"div\",\n",
    "                                                  {\"class\": \"tlid-transliteration-content transliteration-content full\"}]])\n",
    "        translation = \"\".join(translation)\n",
    "        translation_old = translation\n",
    "\n",
    "        # e - ensure that word was translated\n",
    "        if(translation.lower() != english_word):\n",
    "\n",
    "            # f - if there is more than 1 word in the output, choose a random one\n",
    "            translation = translation.split(\" \")\n",
    "            length_translation = len(translation)\n",
    "            if(length_translation == 1):\n",
    "                translation = translation[0]\n",
    "            else:\n",
    "                translation = translation[random.randrange(\n",
    "                    0, length_translation)]\n",
    "\n",
    "            print(\"%i\\t%s\\t\\t|%s\" % (i, english_word, translation.strip()))\n",
    "            chinese_set.add(translation.strip())\n",
    "\n",
    "        else:\n",
    "            print(\"Word \\\"%s\\\" skipped\" % english_word)\n",
    "\n",
    "        # e - clear the input field before rerunning\n",
    "        running.driver.find_element_by_xpath(\"//textarea\").clear()\n",
    "\n",
    "    # 2 - tidy the words and write to file\n",
    "    print(\"-------------------- Section %i/%i--------------------\" %\n",
    "          (section, no_sections_to_split_into))\n",
    "    chinese_set = convert_to_nn_format(chinese_set)\n",
    "    write_set(chinese_set, \"./output/chinese.txt\")\n",
    "\n",
    "    # 3 - reload page to clear cache\n",
    "    running.driver.get(\"https://translate.google.com/\")\n",
    "\n",
    "\n",
    "print(\"==> Translated %i English words to %i Chinese words\" %\n",
    "      (len(english_for_chinese_set), len(chinese_set)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 4 - remove apostrophes from generated file\n",
    "refine_file(\"./output/chinese.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## German Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[171/10000]\tLudwig Forwerk\n",
      "[347/10000]\tTobias Bachmann\n",
      "[367/10000]\tNGC 1534\n",
      "[563/10000]\tRaphael Schall\n",
      "[762/10000]\tHugh van Cutsem\n",
      "[806/10000]\tNational Highway 38\n",
      "[932/10000]\tAndy Borg\n",
      "[1646/10000]\tGeschichte Luxemburgs\n",
      "[1694/10000]\tParadinha\n",
      "[1769/10000]\tCharles Waldron Buckley\n",
      "[1929/10000]\tDennis Wheatley\n",
      "[2047/10000]\tSt. Wendelinus (Zellhausen)\n",
      "[2157/10000]\tAdolf Vögtlin\n",
      "[2168/10000]\tDenkmalgeschützte Objekte im Bezirk Landeck\n",
      "[2718/10000]\tBarthélemy d’Eyck\n",
      "[3381/10000]\tUnited States Air Force Pararescue\n",
      "[3420/10000]\tSteinweg 2 (Quedlinburg)\n",
      "[3462/10000]\tAm Osterberg (Park)\n",
      "[3573/10000]\tPöhlmann KG\n",
      "[3634/10000]\tPolymyxine\n",
      "[3671/10000]\tAlois Puschnik\n",
      "[3728/10000]\tPalmhonig\n",
      "[3771/10000]\tAmt Camen\n",
      "[3914/10000]\tMoodboard\n",
      "[3981/10000]\tMatzen (Bitburg)\n",
      "[4007/10000]\tC.L.E.D.-Agar\n",
      "[4066/10000]\tBurak Karaduman\n",
      "[4081/10000]\tHerbert (Comiczeichner)\n",
      "[4148/10000]\tDänische Superliga 2006/07\n",
      "[4194/10000]\tKronenorden (Belgien)\n",
      "[4239/10000]\tVinson Detenamo\n",
      "[4266/10000]\tUnião das Freguesias de Ceivães e Badim\n",
      "[4565/10000]\tNortorf\n",
      "[4653/10000]\tWilhelm von Hohenau\n",
      "[5005/10000]\tGuido Knopp\n",
      "[5040/10000]\tMarian Suski\n",
      "[5072/10000]\tDialerschutz.de\n",
      "[5102/10000]\tMykola Semena\n",
      "[5162/10000]\tGräberfeld von Annelund\n",
      "[5203/10000]\t(348034) Deslorieux\n",
      "[5222/10000]\tCopi-Naturschutzgebiet\n",
      "[5271/10000]\tElke Christina Roeder\n",
      "[5291/10000]\tConroy Point\n",
      "[5333/10000]\tDahlhausen (Hennef)\n",
      "[5373/10000]\tFunktechnisches Museum im Hessenpark\n",
      "[5419/10000]\tFriedrich Rehm\n",
      "[5507/10000]\tGreta Taubert\n",
      "[6654/10000]\tCommodore VIC 1001, VIC 20, VC 20\n",
      "[7295/10000]\tBajuwaren\n",
      "[7728/10000]\tMasseband\n",
      "[7756/10000]\t3,4′-Bipyridin\n",
      "[7796/10000]\tDhagpo Kagyu Mandala\n",
      "[7831/10000]\tCarl Espen\n",
      "[8297/10000]\tPaul Ryan (Politiker)\n",
      "[8510/10000]\tElm (Schlüchtern)\n",
      "[8576/10000]\tFrew McMillan\n",
      "[8588/10000]\tVerwaltungsgemeinschaft Waldheim\n",
      "[8597/10000]\tThomas Ganske\n",
      "[8721/10000]\tBMW 319/1\n",
      "[9021/10000]\tAnhaltende Trauerstörung\n",
      "[9048/10000]\tFerenc Somodi\n",
      "[9107/10000]\tMedienkonverter\n",
      "[9192/10000]\tInline-Speedskating-Europameisterschaften 2014\n",
      "[9221/10000]\tPatricia Obee\n",
      "[9367/10000]\tStromtalwiese\n",
      "[9406/10000]\tChakachamna Lake\n",
      "[9417/10000]\t(5188) Paine\n",
      "[9474/10000]\tJames Ogilvy, 7. Earl of Findlater\n",
      "[9531/10000]\tNationalbiografie\n",
      "[9575/10000]\tKüppersherweg\n",
      "[9951/10000]\tOmnibusverkehr Franken\n",
      "[10210/10000]\tOberösterreichische Landesbibliothek\n",
      "==> Scraped 72 article(s) for 10210 elements\n",
      "==> Wrote set to file ./output/german.txt.txt\n"
     ]
    }
   ],
   "source": [
    "running.driver.get(\"https://de.wikipedia.org/wiki/Wikipedia:Hauptseite\")\n",
    "target = 10000\n",
    "german_set = set()\n",
    "german_articles = []\n",
    "\n",
    "while(len(german_set) < target):\n",
    "    # collect words from page\n",
    "    wiki_words, title = wikipedia_scrape_words()\n",
    "    german_articles.append(title)\n",
    "    \n",
    "    # filter for length 2-15 and accents and add to set\n",
    "    converted_words = convert_to_nn_format(wiki_words)\n",
    "    german_set = german_set.union(converted_words)\n",
    "\n",
    "    print(\"[%i/%i]\\t%s\" %(len(german_set), target, title))\n",
    "\n",
    "print(\"==> Scraped %i article(s) for %i elements\" %(len(german_articles),len(german_set)))\n",
    "\n",
    "write_set(german_set, \"./output/german.txt\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Random Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Wrote set to file ./output/random.txt.txt\n"
     ]
    }
   ],
   "source": [
    "target = 10000\n",
    "random_set = set()\n",
    "while(len(random_set) < target):\n",
    "\n",
    "    # 1 - generate random word\n",
    "    length = random.randrange(2,15)\n",
    "    random_word = []\n",
    "    \n",
    "    for j in range(0, length):\n",
    "        random_letter =  chr(random.randrange(0,26) + 97)\n",
    "        random_word.append(random_letter)\n",
    "\n",
    "    random_word = \"\".join(random_word)\n",
    "\n",
    "    random_set.add(random_word)\n",
    "write_set(random_set, \"./output/random.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Keymash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85122 keymashes in the file will generate appox. 10014 words\n",
      "**> 85122 keymashes are generating 10000 words\n",
      "\n",
      "\n",
      "==> Wrote set to file ./output/keymash.txt.txt\n"
     ]
    }
   ],
   "source": [
    "target = 10000\n",
    "keymash_set = set()\n",
    "\n",
    "# read the keymash_text file\n",
    "fin = open(\"keymash.txt\")\n",
    "keymash_text = fin.read()\n",
    "fin.close()\n",
    "\n",
    "# keep only the a-z letters\n",
    "keymash_text = re.sub(\"[^a-zA-Z]\",\"\",keymash_text)\n",
    "keymash_text = list(keymash_text)\n",
    "keymash_length = len(keymash_text)\n",
    "print(\"%i keymashes in the file will generate appox. %i words\" %(keymash_length, int(keymash_length/8.5)))\n",
    "\n",
    "i = 0\n",
    "while((len(keymash_set) < target) and (i < keymash_length)):\n",
    "    # 1 - generate random length for word\n",
    "    length = random.randrange(2,15)\n",
    "    keymash_word = []\n",
    "    \n",
    "    for j in range(0, length):\n",
    "        try:\n",
    "            keymash_letter =  keymash_text[i].lower()\n",
    "            keymash_word.append(keymash_letter)\n",
    "            i += 1\n",
    "        except IndexError:\n",
    "            print(\"\\n**> Make more keymashes by SMASHING that keyboard\")\n",
    "            print(\"**> Currently %i keymashes are generating %i words\\n\\n\"\n",
    "                  % (keymash_length, len(keymash_set)))\n",
    "            break\n",
    "        \n",
    "    keymash_word = \"\".join(keymash_word)\n",
    "    keymash_set.add(keymash_word.lower())\n",
    "\n",
    "print(\"**> %i keymashes are generating %i words\\n\\n\"% (keymash_length, len(keymash_set)))\n",
    "\n",
    "write_set(keymash_set, \"./output/keymash.txt\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "name": "nnData_bot.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
